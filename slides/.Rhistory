training
library(ggplot2)
library(rpart)
x <- c(1, 3, 2, 5)
x
x = c(1, 6, 2)
x
x = c(c(1, 6, 2), c(1, 3, 2, 5))
x
x = c(1, 6, 2)
y = c(1, 4, 3)
length(x)
length(y)
x + y
y = c(1, 4, 3, 8)
x + y
y = c(1, 4, 3)
ls()
rm(x, y)
ls()
?matrix
x = matrix(data=c(1, 2, 3, 4), nrow=2, ncol=2)
matrix(data=c(1, 2, 3, 4), nrow=2, ncol=2)
matrix(c(1, 2, 3, 4), 2, 2)
matrix(c(1, 2, 3, 4), 2, 2, byrow=TRUE)
x
sqrt(x)
x^2
rnorm(5)
?rnorm
x = rnorm(50)
y = x + rnorm(50, mean=50, sd=0.1)
x
x[x>0]
length(x[x>0])
length(x[x<=0])
length(x[j<=0])
x<=0
x
y
cor(x, y)
?cor]
?cor
cor(x, y, method="spearman")
set.seed(1303)
rnorm(50)
set.seed(3)
y = rnoem(100)
y = rnorm(100)
mean(y)
var(y)
sd(y)
sqrt(var(y))
x = rnorm(100)
y = rnorm(100)
plot(x, y)
plot(x, y, xlab="X data", ylab="Our Y Axis", main="Random 2d plot")
jpeg("randon-plot-figure.jpeg")
plot(x, y, col="green")
dev.off()
jpeg("randon-plot-figure.jpeg")
plot(x, y, xlab="X data", ylab="Our Y Axis", main="Random 2d plot", col="blue")
dev.off()
x = seq(1, 10)
x
1:10
x = seq(-pi, pi, length)
pi
x
y = x
?outer
f = outer(x, y, function(x, y) cos(y) / (1 + x^2) )
f
contour(x, y, f)
contour(x, y, f, nlevels=45, add=T)
contour(x, y, f)
contour(x, y, f, nlevels=45, add=T)
fa = (f-t(f)) / 2
contour(x, y, fa, nlevels=15)
image(x, y, fa)
persp(x, y, fa)
persp(x, y, fa, theta=30)
persp(x, y, fa, theta=30, phi=20)
persp(x, y, fa, theta=30, phi=40)
persp(x, y, fa, theta=30, phi=70)
persp(x, y, fa, theta=30, phi=40)
image(x, y, fa)
?image
image(x, y, fa, col=‘rainbow’,)
‘heat.colors’, ‘topo.colors’, ‘terrain.colors’
image(x, y, fa, col=rainbow)
image(x, y, fa, col="rainbow")
image(x, y, fa, col=topo.colors)
image(x, y, fa, col="topo.colors")
A = matrix(1:16, 4, 4)
1:16
A
A[2, 3]
A[3, 2]
A[1:3, c(2, 4)]‘rainbow’,
A[1:3, c(2, 4)]
A
A[1:2, ]
A[, 1:2]
A[1,]
A[-c(1, 3),]
c(1, 3)
A
A[-c(1, 3), -c(1, 3, 4)]
dim(A[-c(1, 3), -c(1, 3, 4)])
dim(A)
q()
library(ISLR)
summary(Hitters)
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
plot(regfit.full,scale="Cp")
coef(regfit.full,10)
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
rep(NA, 5)
val.errors=rep(NA,19)
x.test=model.matrix(Salary~.,data=Hitters[-train,])# notice the -index!
for(i in 1:19){
coefi=coef(regfit.fwd,id=i)
pred=x.test[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==k,],id=i)
cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
}
}
predict.regsubsets=function(object,newdata,id,...){
form=as.formula(object$call[[2]])
mat=model.matrix(form,newdata)
coefi=coef(object,id=id)
mat[,names(coefi)]%*%coefi
}
cv.errors=matrix(NA,10,19)
set.seed(11)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
for(i in 1:19){
pred=predict(best.fit,Hitters[folds==k,],id=i)
cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
}
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")
library(glmnet)
install.packages("glmnet")
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
x
y
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar="lambda",label=TRUE)
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
names(cv.ridge)
?cv.glmnet
cv.ridge$cvm
plot(cv.ridge)
fit.lasso=glmnet(x,y)
plot(fit.lasso,xvar="lambda",label=TRUE)
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
plot(fit.lasso,xvar="lambda",label=TRUE)
lasso.tr=glmnet(x[train,],y[train])
lasso.tr
pred=predict(lasso.tr,x[-train,])
dim(pred)
rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
require(randomForest)
?randomForest
DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))
prcomp(DF, scale = FALSE)
cov(c(80, 90, 95), c(60, 70, 40))
cov(c(55, 45, 50), c(60, 70, 40))
cov(c(1, 2, 3), c(1, 2, 3))
?cov
0.71 * 0.71
?rbind
data1 = load("10.R.RData")
cwd()
setwd("~/projects/R/stanford_statistical_learning/slides")
data1 = load("10.R.RData")
load("10.R.RData")
load("10.R.RData.1")
rbind(x, x.test)
dim(x, x.test)
dim(x)
dim(x.test)
all_x = rbind(x, x.test)
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2, var)
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
names(pca.out)
biplot(pca.out, scale=0)
all_x_pca = prcomp(all_x, scale=T)
all_x_pca
names(all_x_pca)
?prcomp
?factanal
summary(all_x_pca)
